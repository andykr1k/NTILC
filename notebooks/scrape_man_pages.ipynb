{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6577dbff",
   "metadata": {},
   "source": [
    "# Simple man7 scrape with BeautifulSoup\n",
    "\n",
    "Fetch the index page and return links whose anchor text ends with `(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc8ecb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "OUTPUT_PATH = '/scratch4/home/akrik/NTILC/data/man/raw.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "_BULLET_RE = re.compile(r\"^\\s*•\\s*(.+?)\\s*$\")\n",
    "_ARG_SAME_LINE_RE = re.compile(r\"(\\s+<[^>]+>\\s*)$\")\n",
    "\n",
    "def _find_section_pre(soup: BeautifulSoup, section_id: str):\n",
    "    a = soup.find(\"a\", id=section_id)\n",
    "    if not a:\n",
    "        return None\n",
    "    h2 = a.find_parent(\"h2\")\n",
    "    if not h2:\n",
    "        return None\n",
    "    return h2.find_next_sibling(\"pre\")\n",
    "\n",
    "def _clean_pre_text(pre) -> str:\n",
    "    if pre is None:\n",
    "        return \"\"\n",
    "    txt = pre.get_text(\"\\n\")\n",
    "    lines = [ln.rstrip() for ln in txt.splitlines()]\n",
    "    i, j = 0, len(lines)\n",
    "    while i < j and not lines[i].strip():\n",
    "        i += 1\n",
    "    while j > i and not lines[j - 1].strip():\n",
    "        j -= 1\n",
    "    return \"\\n\".join(lines[i:j])\n",
    "\n",
    "def _parse_name_section(name_text: str) -> Dict[str, str]:\n",
    "    lines = [ln.strip() for ln in name_text.splitlines()]\n",
    "    nonempty = [ln for ln in lines if ln]\n",
    "\n",
    "    command = short = desc = \"\"\n",
    "    if nonempty:\n",
    "        m = re.match(r\"^(\\S+)\\s*-\\s*(.+)$\", nonempty[0])\n",
    "        if m:\n",
    "            command = m.group(1).strip()\n",
    "            short = m.group(2).strip()\n",
    "\n",
    "            orig = name_text.splitlines()\n",
    "            first_idx = next((i for i, ln in enumerate(orig) if ln.strip() == nonempty[0]), 0)\n",
    "            rest = orig[first_idx + 1 :]\n",
    "            while rest and not rest[0].strip():\n",
    "                rest.pop(0)\n",
    "            desc = \"\\n\".join(r.rstrip() for r in rest).strip()\n",
    "        else:\n",
    "            desc = name_text.strip()\n",
    "\n",
    "    return {\"command\": command, \"short_description\": short, \"description\": desc}\n",
    "\n",
    "def _parse_invocation_section(inv_text: str) -> str:\n",
    "    lines = [ln.strip() for ln in inv_text.splitlines() if ln.strip()]\n",
    "    s = \" \".join(lines)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "_OPT_START_RE = re.compile(r\"^\\s*(?:•|Â•)\\s+(.+?)\\s*$|^\\s*(--?\\S.*)$\")\n",
    "\n",
    "_ARG_SAME_LINE_RE = re.compile(r\"(\\s+<[^>]+>\\s*)$\")\n",
    "\n",
    "def _split_option_blocks(options_text: str):\n",
    "    lines = options_text.splitlines()\n",
    "\n",
    "    starts = []\n",
    "    for i, ln in enumerate(lines):\n",
    "        if _OPT_START_RE.match(ln):\n",
    "            starts.append(i)\n",
    "\n",
    "    if not starts:\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    for k, s in enumerate(starts):\n",
    "        e = starts[k + 1] if k + 1 < len(starts) else len(lines)\n",
    "        blocks.append(lines[s:e])\n",
    "    return blocks\n",
    "\n",
    "def _parse_option_header(header_line: str):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      • --help\n",
    "      Â• --help\n",
    "      --help\n",
    "      -h\n",
    "      --version | -v\n",
    "    \"\"\"\n",
    "    m = _OPT_START_RE.match(header_line)\n",
    "    if not m:\n",
    "        return [], \"\"\n",
    "\n",
    "    head = (m.group(1) or m.group(2) or \"\").strip()\n",
    "\n",
    "    arg = \"\"\n",
    "    arg_m = _ARG_SAME_LINE_RE.search(head)\n",
    "    if arg_m:\n",
    "        arg = arg_m.group(0).strip()\n",
    "        head = head[:arg_m.start()].strip()\n",
    "\n",
    "    parts = [p.strip() for p in head.split(\"|\")]\n",
    "    flags = [re.sub(r\"\\s+\", \" \", p).strip() for p in parts if p]\n",
    "    return flags, arg\n",
    "\n",
    "def _parse_option_block(block_lines: List[str]) -> Dict[str, str]:\n",
    "    header = block_lines[0]\n",
    "    flags, arg = _parse_option_header(header)\n",
    "\n",
    "    rest = block_lines[1:]\n",
    "\n",
    "    if not arg:\n",
    "        for idx in range(min(3, len(rest))):\n",
    "            s = rest[idx].strip()\n",
    "            if re.fullmatch(r\"<[^>]+>\", s):\n",
    "                arg = s\n",
    "                rest = rest[:idx] + rest[idx+1:]\n",
    "                break\n",
    "\n",
    "    i = 0\n",
    "    while i < len(rest) and not rest[i].strip():\n",
    "        i += 1\n",
    "    rest = rest[i:]\n",
    "\n",
    "    paragraphs = []\n",
    "    cur = []\n",
    "    for ln in rest:\n",
    "        if not ln.strip():\n",
    "            if cur:\n",
    "                paragraphs.append(\" \".join(cur).strip())\n",
    "                cur = []\n",
    "            continue\n",
    "        cur.append(re.sub(r\"\\s+\", \" \", ln.strip()))\n",
    "    if cur:\n",
    "        paragraphs.append(\" \".join(cur).strip())\n",
    "\n",
    "    return {\n",
    "        \"flags\": flags,\n",
    "        \"arg\": arg,\n",
    "        \"description\": \"\\n\\n\".join(paragraphs).strip(),\n",
    "    }\n",
    "\n",
    "def _parse_options_section(options_text: str) -> List[Dict[str, Any]]:\n",
    "    blocks = _split_option_blocks(options_text)\n",
    "    return [_parse_option_block(b) for b in blocks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b37a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128 Safari/537.36\"\n",
    "}\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def _fetch_html_cached(url: str, timeout: float = 15.0) -> str:\n",
    "\n",
    "    with requests.Session() as s:\n",
    "        s.headers.update(_DEFAULT_HEADERS)\n",
    "        r = s.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "\n",
    "def fetch_html(url: str, *, session=None, timeout: float = 15.0, use_cache: bool = True) -> str:\n",
    "    if use_cache and session is None:\n",
    "        return _fetch_html_cached(url, timeout=timeout)\n",
    "\n",
    "    s = session or requests.Session()\n",
    "    if \"User-Agent\" not in s.headers:\n",
    "        s.headers.update(_DEFAULT_HEADERS)\n",
    "\n",
    "    r = s.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    r.encoding = \"utf-8\"\n",
    "    return r.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_man7_page(url: str, *, session: Optional[requests.Session] = None, timeout: float = 15.0, use_cache: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch + parse a man7.org HTML manpage into:\n",
    "      { name, one_line, description, invocation, options }\n",
    "\n",
    "    Efficient choices:\n",
    "      - requests.Session reuse (if you pass session)\n",
    "      - optional LRU cache for repeated URLs\n",
    "      - only parses the three sections you asked for (NAME/INVOCATION/OPTIONS)\n",
    "    \"\"\"\n",
    "    html = fetch_html(url, session=session, timeout=timeout, use_cache=use_cache)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    name_text = _clean_pre_text(_find_section_pre(soup, \"NAME\"))\n",
    "    inv_text  = _clean_pre_text(_find_section_pre(soup, \"INVOCATION\"))\n",
    "    opt_text  = _clean_pre_text(_find_section_pre(soup, \"OPTIONS\"))\n",
    "\n",
    "    name_parsed = _parse_name_section(name_text)\n",
    "    invocation = _parse_invocation_section(inv_text) if inv_text else \"\"\n",
    "\n",
    "    record = {\n",
    "        \"name\": name_parsed[\"command\"],\n",
    "        \"one_line\": name_parsed[\"short_description\"],\n",
    "        \"description\": name_parsed[\"description\"],\n",
    "        \"invocation\": invocation,\n",
    "        \"options\": _parse_options_section(opt_text) if opt_text else [],\n",
    "        \"source_url\": url,\n",
    "    }\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6ae3da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://man7.org/linux/man-pages/dir_all_alphabetic.html\"\n",
    "response = requests.get(url, timeout=30)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "pattern = re.compile(r\"\\(1\\)$\")\n",
    "\n",
    "section_1_links = [\n",
    "    urljoin(url, a.get(\"href\"))\n",
    "    for a in soup.find_all(\"a\")\n",
    "    if a.get_text(strip=True) and pattern.search(a.get_text(strip=True))\n",
    "]\n",
    "\n",
    "len(section_1_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a30ea170",
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as session, open(OUTPUT_PATH, \"w\") as f:\n",
    "    f.write(\"[\\n\")\n",
    "\n",
    "    for i, url in enumerate(section_1_links):\n",
    "        rec = parse_man7_page(url, session=session, use_cache=False)\n",
    "\n",
    "        json.dump(rec, f, indent=2)\n",
    "\n",
    "        # Add comma except after last element\n",
    "        if i < len(section_1_links) - 1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
